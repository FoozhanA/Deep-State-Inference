{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import dataset_ops\n",
    "import functools\n",
    "from transfer_learning import evaluate_model\n",
    "import numpy as np  # noqa\n",
    "import matplotlib.pyplot as plt  # noqa\n",
    "from model_helper import make_model\n",
    "from metrics import F1, Precision, Recall, soft_dice_loss, remove_clutter_one_sample, ClassPrecision, ClassRecall\n",
    "import datetime\n",
    "import cuda\n",
    "import pandas_format  # noqa\n",
    "from pathlib import Path\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "try:\n",
    "    from tqdm import notebook as tqdm\n",
    "except ImportError:\n",
    "    tqdm = None\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "cuda.initialize()\n",
    "\n",
    "\n",
    "dataset_manager = dataset_ops.MicroPilotTestsManager(dataset_dir=Path('h5'), runs_filename='runs.hdf')\n",
    "# dataset_manager = dataset_ops.PaparazziTestManager(dataset_dir=Path('pprz_h5'), runs_filename='pprz_runs.hdf')\n",
    "all_runs = dataset_manager.get_all_available_tests()\n",
    "\n",
    "\n",
    "selected_runs = all_runs.loc[(all_runs['Test Length'] > 200) & (all_runs['Test Length'] < 20000)]\n",
    "# selected_runs = selected_runs.iloc[:40]\n",
    "# selected_runs = all_runs.sample(frac=1, axis=1, random_state=55)\n",
    "# tl_plot = selected_runs['Test Length'].plot(kind='hist', bins=25, figsize=[10,5])\n",
    "# tl_plot.tick_params(labelsize=14)\n",
    "# tl_plot.set_xlim([10,18000])\n",
    "# tl_plot.set_xlabel('Test Length ($l_k$)', fontsize=15)\n",
    "# tl_plot.set_ylabel('Number of Tests', fontsize=15)\n",
    "# tl_plot.figure.savefig('paper_data/test_lengths.png')\n",
    "# #selected_runs\n",
    "# print(all_runs.shape, selected_runs.shape)\n",
    "# selected_runs['Test Length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputs = ('SpeedFts', 'Pitch', 'Roll', 'Yaw', 'current_altitude', )\n",
    "outputs= ('elev', 'ai', 'rdr', 'throttle', 'Flaps')\n",
    "\n",
    "max_length = 18000\n",
    "\n",
    "tfdataset = dataset_ops.TensorflowDataset(dataset_manager)\n",
    "train_dataset, test_dataset, validation_dataset = dataset_ops.split_dataset(\n",
    "    tfdataset.get_dataset(selected_runs, features=inputs+outputs, max_length=max_length),\n",
    "    split_proportion=(6, 1, 3)\n",
    ")  # 60% 10% 30% += 100%\n",
    "train_dataset, test_dataset, validation_dataset = (\n",
    "dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "        .batch(25)\n",
    "        .shuffle(buffer_size=15)\n",
    "        for dataset in (train_dataset, test_dataset, validation_dataset)\n",
    ")\n",
    "\n",
    "assert dataset_manager.count_states() > 0\n",
    "\n",
    "train_dataset.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def create_prec_recall_f1(tolerance):\n",
    "    prec = Precision(name=f'prec_{tolerance}', tolerance=tolerance)\n",
    "    recl = Recall(name=f'recall_{tolerance}', tolerance=tolerance)\n",
    "\n",
    "    return [\n",
    "        # ClassPrecision(),\n",
    "        # ClassRecall(),\n",
    "        prec,\n",
    "        recl,\n",
    "        F1(prec, recl),\n",
    "    ]\n",
    "\n",
    "\n",
    "evaluation_metrics = create_prec_recall_f1(25)\n",
    "metrics_reporting = (create_prec_recall_f1(5)[:-1] +\n",
    "                        create_prec_recall_f1(15)[:-1] +\n",
    "                        create_prec_recall_f1(25)[:-1] +\n",
    "                        [ClassPrecision(), ClassRecall()])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=3e-5)\n",
    "\n",
    "# MP:\n",
    "mp_model_builder = functools.partial(make_model, inputs, outputs, max_length, n_states=dataset_manager.count_states())\n",
    "\n",
    "full_model = mp_model_builder(convs=[(64, 3), (64, 5), (64, 10), (64, 15), (64, 20)], grus=[128, 128], name='mp_model')\n",
    "full_model.summary()\n",
    "full_model.compile(loss=soft_dice_loss, optimizer=optimizer, metrics=evaluation_metrics)\n",
    "\n",
    "cnn_baseline_model = mp_model_builder(convs=[(64, 3), (64, 5), (64, 10), (64, 15), (64, 20)], grus=[], name='convolutional_baseline')\n",
    "cnn_baseline_model.summary()\n",
    "cnn_baseline_model.compile(loss=soft_dice_loss, optimizer=optimizer, metrics=evaluation_metrics)\n",
    "\n",
    "rnn_baseline_model = mp_model_builder(convs=[(1, 1)], grus=[128, 128], name='recurrent_baseline')\n",
    "rnn_baseline_model.summary()\n",
    "rnn_baseline_model.compile(loss=soft_dice_loss, optimizer=optimizer, metrics=evaluation_metrics)\n",
    "\n",
    "evaluation_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "# epochs = 5\n",
    "\n",
    "for model_name, model in zip(('full', 'rnn', 'cnn',), (full_model, rnn_baseline_model, cnn_baseline_model)):\n",
    "    training_start_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    log_dir=\"logs/fit/\" + training_start_time\n",
    "    file_name = f'models/mp_cameraready-{model_name}-{training_start_time}-{epochs}.h5'\n",
    "    if Path(file_name).exists():\n",
    "        model.load_weights(file_name)\n",
    "    else:\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "        history = model.fit(train_dataset,\n",
    "                            epochs=epochs,\n",
    "                            validation_data=validation_dataset,\n",
    "                            callbacks=[\n",
    "                                tensorboard_callback,\n",
    "                                tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "                            ])\n",
    "        model.save(file_name)\n",
    "        tf.keras.utils.plot_model(model, show_shapes=True, to_file=file_name.replace('.h5', '.png'))\n",
    "\n",
    "    evaluation_results[model_name] = evaluate_model(model, validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rq3 = pd.DataFrame(evaluation_results).set_index(pd.Index(\n",
    "    ['prec_5', 'recall_5', 'prec_15', 'recall_15', 'prec_25', 'recall_25', 'class_precision', 'class_recall'])).T\n",
    "for _tau in [5, 15, 25]:\n",
    "    p = rq3[f'prec_{_tau}']\n",
    "    r = rq3[f'recall_{_tau}']\n",
    "    f1 = 2 * p * r / (p + r)\n",
    "    rq3.insert(rq3.columns.to_list().index(f'recall_{_tau}') + 1, f\"F1_{_tau}\", f1)\n",
    "\n",
    "p, r = rq3['class_precision'], rq3['class_recall']\n",
    "rq3['class_F1'] = 2 * p * r / (p + r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with pandas_format.PandasFloatFormatter('{:,.2f}%'):\n",
    "    print((rq3*100).T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(15, 2, figsize=(15, 8), sharex=True)\n",
    "# N = 2500\n",
    "N = 1000\n",
    "\n",
    "folder_name = Path('plots') / 'output_compare' / ('categorical_' + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "if not folder_name.exists():\n",
    "    folder_name.mkdir(parents=True)\n",
    "\n",
    "print('Plotting in', folder_name)\n",
    "\n",
    "results = []\n",
    "for set_name, data_set in (('Training', train_dataset), ('Test', test_dataset), ('Validation', validation_dataset)):\n",
    "    for bi, data in data_set.unbatch().batch(30).enumerate():\n",
    "        ins, ground_truth = data\n",
    "        prediction = model.predict_on_batch(ins)\n",
    "        mask = tf.squeeze(ins['mask'], axis=-1)\n",
    "        ground_truth = tf.squeeze(ground_truth)\n",
    "\n",
    "        for metric in metrics_reporting: \n",
    "            metric.reset_states()\n",
    "            metric.update_state(ground_truth, prediction)\n",
    "        \n",
    "        prediction = tf.math.argmax(prediction, axis=-1)\n",
    "        no_clutter = tf.map_fn(remove_clutter_one_sample, prediction)\n",
    "        ground_truth = tf.math.argmax(ground_truth, axis=-1)\n",
    "        \n",
    "        run_length = tf.math.minimum(tf.argmin(mask, axis=-1), N)\n",
    "        max_run_length_in_batch = int(tf.math.reduce_max(run_length))\n",
    "\n",
    "        results.append(\n",
    "            [set_name] + [float(metric.result()) for metric in metrics_reporting]  \n",
    "        )\n",
    "\n",
    "        for prednc, truth, idx, ax in zip(no_clutter, ground_truth, run_length, axs.reshape(-1)):\n",
    "            if idx == 0:\n",
    "                idx = N\n",
    "            truth, prednc = truth[:idx], prednc[:idx]\n",
    "\n",
    "            concat = tf.stack((prednc, truth), axis=0)\n",
    "            # ax.imshow(concat, aspect='auto', interpolation='nearest', vmin=0, vmax=dataset_manager.count_states())\n",
    "            ax.imshow(concat, aspect='auto', interpolation='nearest')#, vmin=0, vmax=dataset_manager.count_states())\n",
    "            ax.set_yticklabels(['', '$\\\\hat{O}$', '$O$'])\n",
    "            ax.set_xlim([1, max_run_length_in_batch])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(folder_name / f'{set_name}_{bi}.png')\n",
    "        for ax in axs.reshape(-1): ax.clear()\n",
    "plt.close()\n",
    "columns = ['Dataset'] + [metric.name for metric in metrics_reporting] \n",
    "results = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "# results['class_precision'] *= 100\n",
    "# results['class_recall'] *= 100\n",
    "results['class_F1'] = 2*results['class_precision']*results['class_recall'] / (results['class_precision']+results['class_recall'])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = results.groupby('Dataset').aggregate('mean')*100\n",
    "for _tau in [5, 15, 25]:\n",
    "    p = df[f'prec_{_tau}']\n",
    "    r = df[f'recall_{_tau}']\n",
    "    f1 = 2 * p * r / (p + r)\n",
    "    df.insert(df.columns.to_list().index(f'recall_{_tau}') + 1, f\"F1_{_tau}\", f1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_results = df\n",
    "\n",
    "with pandas_format.PandasFloatFormatter('{:,.2f}%'):\n",
    "    print(paper_results.loc[['Validation']].T.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with pandas_format.PandasFloatFormatter('{:,.2f}%'):\n",
    "    print(paper_results.loc[['Validation', 'Test', 'Training'], ~paper_results.columns.str.contains('class_')].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "with pandas_format.PandasFloatFormatter('{:,.2f}%'):\n",
    "    display(paper_results.loc[['Validation']].T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('mp': venv)",
   "language": "python",
   "name": "python36964bitmpvenvc0be145795914919ad5cb1b44742ad1c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
