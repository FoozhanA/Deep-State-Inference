{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "\n",
    "try:\n",
    "    from tqdm import notebook as tqdm\n",
    "except ImportError:\n",
    "    tqdm = None\n",
    "    \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import dataset_ops\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)  # show all columns\n",
    "GPUs = tf.config.list_physical_devices('GPU')\n",
    "if GPUs is None or len(GPUs) == 0:\n",
    "    print(\"WARNING: No GPU, all there is is:\")\n",
    "    for device in tf.config.list_physical_devices():\n",
    "        print(f'- {device}')\n",
    "else:\n",
    "    for gpu in GPUs:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"Initialized\", gpu)\n",
    "\n",
    "dataset_manager = dataset_ops.TestsManager(dataset_dir='./h5', runs_filename='runs.hdf')\n",
    "all_runs = dataset_manager.get_all_available_tests()\n",
    "\n",
    "selected_runs = all_runs.loc[(all_runs['Test Length'] > 200) & (all_runs['Test Length'] < 20000)]\n",
    "# selected_runs = selected_runs.iloc[:40]\n",
    "plt = selected_runs['Test Length'].plot(kind='hist', bins=25, figsize=[10,5])\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.set_xlim([10,18000])\n",
    "plt.set_xlabel('Test Length ($l_k$)', fontsize=15)\n",
    "plt.set_ylabel('Number of Tests', fontsize=15)\n",
    "plt.figure.savefig('test_lengths.png')\n",
    "# #selected_runs\n",
    "# print(all_runs.shape, selected_runs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inputs = ('SpeedFts', 'Pitch', 'Roll', 'Yaw', 'current_altitude', )\n",
    "outputs= ('elev', 'ai', 'rdr', 'throttle', 'Flaps')\n",
    "\n",
    "# max_length = selected_runs['Test Length'].max()\n",
    "max_length = 18000 \n",
    "\n",
    "\n",
    "tfdataset = dataset_ops.TensorflowDataset(dataset_manager)\n",
    "dataset = tfdataset.get_dataset(selected_runs, batch_size=25, features=inputs+outputs, max_length=max_length)\n",
    "dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "test_dataset = dataset.enumerate().filter(lambda x,y: x % 20 == 0).map(lambda x,y: y)\n",
    "validation_dataset = dataset.enumerate().filter(lambda x,y: x % 20 == 1).map(lambda x,y: y)\n",
    "train_dataset = dataset.enumerate().filter(lambda x,y: x % 20 > 1).map(lambda x,y: y)\n",
    "\n",
    "dataset.element_spec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model_helper import MaskStealingLayer\n",
    "\n",
    "def make_model(inputs, outputs, input_length):\n",
    "    n_in = len(inputs)\n",
    "    n_out = len(outputs)\n",
    "    n_features = n_in + n_out\n",
    "    \n",
    "    bias_initializer = tf.keras.initializers.Constant(np.log(0.01))\n",
    "    \n",
    "    signals = tf.keras.Input(shape=[input_length, n_features], name='signals')\n",
    "    mask = tf.keras.Input(shape=[input_length, 1], name='mask')\n",
    "    \n",
    "    x = signals\n",
    "    x = MaskStealingLayer(0)((x, mask))\n",
    "    \n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=3, padding=\"same\", name='conv_3')(x)\n",
    "    # x = tf.keras.layers.MaxPool1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=5, padding=\"same\", name='conv_5')(x)\n",
    "#     x = tf.keras.layers.MaxPool1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=10, padding=\"same\", name='conv_10')(x)\n",
    "#     x = tf.keras.layers.MaxPool1D(pool_size=2)(x)\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=15, padding=\"same\", name='conv_15')(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv1D(filters=64, kernel_size=20, padding=\"same\", name='conv_20')(x)\n",
    "#     x = tf.keras.layers.MaxPool1D(pool_size=2)(x)\n",
    "    \n",
    "    x = tf.keras.layers.GRU(128, return_sequences=True)(x)\n",
    "#     x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.GRU(128, return_sequences=True)(x)\n",
    "#     x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(128)(x)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(dataset_manager.count_states(), bias_initializer=bias_initializer, activation='softmax')(x)\n",
    "\n",
    "    # x = tf.keras.layers.UpSampling1D(2 ** 1)(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[signals, mask], outputs=x)\n",
    "    \n",
    "    return model\n",
    "\n",
    "# %%\n",
    "\n",
    "from metrics import F1, Precision, Recall, soft_dice_loss, remove_clutter_one_sample, ClassPrecision, ClassRecall\n",
    "\n",
    "def create_prec_recall_f1(tolerance):\n",
    "    prec = Precision(name=f'prec_{tolerance}', tolerance=tolerance)\n",
    "    recl = Recall(name=f'recall_{tolerance}', tolerance=tolerance)\n",
    "\n",
    "    return [\n",
    "        # ClassPrecision(),\n",
    "        # ClassRecall(),\n",
    "        prec,\n",
    "        recl,\n",
    "        F1(prec, recl),\n",
    "    ]\n",
    "\n",
    "evaluation_metrics = create_prec_recall_f1(25)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=3e-5)\n",
    "\n",
    "model = make_model(inputs, outputs, max_length)\n",
    "model.compile(loss=soft_dice_loss, optimizer=optimizer, metrics=evaluation_metrics)\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "# epochs = 5\n",
    "\n",
    "training_start_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir=\"logs/fit/\" + training_start_time\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "history = model.fit(train_dataset,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=validation_dataset,\n",
    "                    callbacks=[\n",
    "                        tensorboard_callback,\n",
    "                        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5),\n",
    "                    ])\n",
    "model.save(f'models/categroical-{training_start_time}-{epochs}.h5')\n",
    "tf.keras.utils.plot_model(model, show_shapes=True, to_file=f'models/categorical-{training_start_time}-{epochs}.png')\n",
    "model.evaluate(test_dataset)\n",
    "# history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(model.evaluate(test_dataset))\n",
    "print(model.evaluate(validation_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights('models/categroical-20200325-225905-100.h5')\n",
    "# model.load_weights('models/categroical-20200507-225905-100.h5')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(15, 2, figsize=(15, 8), sharex=True)\n",
    "N = 600\n",
    "\n",
    "folder_name = f'Batch/categorical_{datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")}'\n",
    "# os.mkdir(folder_name)\n",
    "print('Plotting in', folder_name)\n",
    "\n",
    "metrics_reporting = evaluation_metrics[:-1] + [\n",
    "    Precision(name='prec_15', tolerance=15),\n",
    "    Recall(name='recall_15', tolerance=15),\n",
    "    Precision(name='prec_5', tolerance=5),\n",
    "    Recall(name='recall_5', tolerance=5),\n",
    "    # ClassPrecision(),\n",
    "    # ClassRecall(),\n",
    "]\n",
    "results = []\n",
    "for set_name, data_set in (('Training', train_dataset), ('Test', test_dataset)):\n",
    "    for bi, data in data_set.unbatch().batch(30).enumerate():\n",
    "        ins, gt = data\n",
    "        prediction = model.predict_on_batch(ins)\n",
    "        mask = tf.squeeze(ins['mask'], axis=-1)\n",
    "        gt = tf.squeeze(gt)\n",
    "\n",
    "        for metric in metrics_reporting: \n",
    "            metric.reset_states()\n",
    "            metric.update_state(gt, prediction)\n",
    "        \n",
    "        prediction = tf.math.argmax(prediction, axis=-1)\n",
    "        no_clutter = tf.map_fn(remove_clutter_one_sample, prediction)\n",
    "        gt = tf.math.argmax(gt, axis=-1)\n",
    "        \n",
    "        run_length = tf.math.minimum(tf.argmin(mask, axis=-1), N)\n",
    "        \n",
    "        results.append(\n",
    "            [set_name] + [float(metric.result()) for metric in metrics_reporting]  \n",
    "        )\n",
    "        \n",
    "        # for prednc, truth, idx, ax in zip(no_clutter, gt, run_length, axs.reshape(-1)):\n",
    "        #     if idx == 0:\n",
    "        #         idx = N\n",
    "        #     truth, prednc = truth[:idx], prednc[:idx]\n",
    "        # \n",
    "        #     concat = tf.stack((prednc, truth), axis=0)\n",
    "        #     ax.imshow(concat, aspect='auto', interpolation='nearest')\n",
    "        #     ax.set_yticklabels(['', '$\\\\hat{O}$', '$O$'])\n",
    "        #     ax.set_xlim([1, N])\n",
    "        # \n",
    "        # plt.tight_layout()\n",
    "        # fig.savefig(f'{folder_name}/{set_name}_{bi}.png')\n",
    "        # for ax in axs.reshape(-1): ax.clear()\n",
    "plt.close()\n",
    "columns = ['Dataset'] + [metric.name for metric in metrics_reporting] \n",
    "results = pd.DataFrame(results, columns=columns)\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "\n",
    "# results['class_precision'] *= 100\n",
    "# results['class_recall'] *= 100\n",
    "# results['F1'] = 2*results['class_precision']*results['class_recall'] / (results['class_precision']+results['class_recall'])\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = results.groupby('Dataset').aggregate('mean')*100\n",
    "for _tau in [5, 15, 25]:\n",
    "    p = df[f'prec_{_tau}']\n",
    "    r = df[f'recall_{_tau}']\n",
    "    f1 = 2 * p * r / (p + r)\n",
    "    df.insert(df.columns.to_list().index(f'recall_{_tau}') + 1, f\"F1_{_tau}\", f1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import minmax_scale\n",
    "# plt.plot(train_dataset[0]['signals'])\n",
    "i = [*train_dataset.unbatch().take(1).as_numpy_iterator()][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inp = model.input\n",
    "outputs = [layer.output for layer in model.layers[3:]]\n",
    "functors = [tf.keras.backend.function([inp], [output]) for output in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputs=[*train_dataset.unbatch().batch(1).take(1).as_numpy_iterator()]\n",
    "layer_outs = [func(inputs) for func in functors]\n",
    "layer_outs\n",
    "# conv1 = functors[0]\n",
    "# conv1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(minmax_scale(layer_outs[9][0][0][150:900,-4]))\n",
    "plt.plot(minmax_scale(layer_outs[9][0][0][150:900,-3]) + 1)\n",
    "plt.plot(minmax_scale(layer_outs[9][0][0][150:900,-5]) + 2)\n",
    "plt.plot(minmax_scale(layer_outs[9][0][0][150:900,-8]) + 3)\n",
    "plt.axis('off')\n",
    "plt.savefig('signal-out.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*enumerate(model.layers[3:])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(minmax_scale(i['signals'][150:900,-4]))\n",
    "plt.plot(minmax_scale(i['signals'][150:900,-3]) + 1)\n",
    "plt.plot(minmax_scale(i['signals'][150:900,-5]) + 2)\n",
    "plt.plot(minmax_scale(i['signals'][150:900,-8]) + 3)\n",
    "plt.axis('off')\n",
    "plt.savefig('signal.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('mp': venv)",
   "language": "python",
   "name": "python36964bitmpvenvc0be145795914919ad5cb1b44742ad1c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}